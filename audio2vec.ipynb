{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tqdm import tqdm\nimport os\nimport librosa\nimport numpy as np\n\ndef load_and_process_audio(file_path, max_pad_len=None):\n    # load a .wav file with sampling rate\n    audio, sr = librosa.load(file_path, sr=22050)\n    \n    # spectrogram\n    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n    \n    # convert to db\n    S_dB = librosa.power_to_db(S, ref=np.max)\n    \n    # padding & tripping the calculated spectrogram!\n    if max_pad_len:\n        # this part confused me at first\n        # we are just normalizing! \n        # making it smaller or larger as needed\n        pad_width = max_pad_len - S_dB.shape[1]\n        # pad if smaller\n        if pad_width > 0:\n            S_dB = np.pad(S_dB, pad_width=((0, 0), (0, pad_width)), mode='constant')\n        # trim if larger\n        else:\n            S_dB = S_dB[:, :max_pad_len]\n    \n    # normalize dimensions\n    S_dB = S_dB[..., np.newaxis]\n    return S_dB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from dataset\nmax_pad_len = 228","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(directory, max_pad_len):\n    # assign data & labels\n    data = []\n    labels = []\n    for subdir in tqdm(os.listdir(directory)):\n        subdir_path = os.path.join(directory, subdir)\n        if os.path.isdir(subdir_path):\n            # walk through all (thanks cursor for the help here)\n            for filename in os.listdir(subdir_path):\n                \n                file_path = os.path.join(subdir_path, filename)\n                \n                if os.path.isfile(file_path) and file_path.endswith('.wav'):\n                    spectrogram = load_and_process_audio(file_path, max_pad_len)\n                    # based on filename format from dataset\n                    emotion = filename.split('-')[2]\n                    \n                    data.append(spectrogram)\n                    labels.append(emotion)\n                    \n    return np.array(data), np.array(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\ndef build_model(input_shape, dimensions):\n    model = tf.keras.Sequential([\n        # lots of stuff happening here!\n        # we start with messy spectrogram data, but want to end up with a dimensional space\n        \n        # first round of feature extraction\n        # capture low-level spatial features (ie textures)\n        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n\n        # reduces spatial dimensions for next layer\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n\n        # second round of feature extraction, more complex\n        # capture more complex spatial features\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n\n        # reduce spatial dimensions again\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        \n        # now in the format we need\n        # convert 2d feature maps into 1d\n        tf.keras.layers.Flatten(),\n        \n        # 1D representation -> embedding\n        # represent embedding as dimensional vector\n        tf.keras.layers.Dense(dimensions, activation='relu'),\n        \n        # different classifications of emotions (we have 8 of them!)\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\nX, y = load_data('ravdess/', max_pad_len)\nX.shape, X[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# similar approach to contrastive learning!\n# we want to encode data based on our labels\n# that's kinda the point of this all\n\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)\ny_categorical = to_categorical(y_encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (128, max_pad_len, 1)\nnum_classes = len(np.unique(y_encoded))\ndimensions = 256\n\nmodel = build_model(input_shape, dimensions)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X, y_categorical, epochs=10, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# given from dataset!\n\nemotion_mapping = {\n    1: 'neutral',\n    2: 'calm',\n    3: 'happy',\n    4: 'sad',\n    5: 'angry',\n    6: 'fearful',\n    7: 'disgust',\n    8: 'surprised'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n# initialize model to extract features like input and layers\n_ = model.predict(X)\n\n# dense layer\nembeddings = model.layers[-2].output\n\n# run model on our initial data\nmodel_embedding = tf.keras.Model(inputs=model.input, outputs=embeddings)\nX_embed = model_embedding.predict(X)\n\n# reduce down to 2dim space\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced = tsne.fit_transform(X_embed)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_encoded, cmap='viridis')\n\ncbar = plt.colorbar(scatter, ticks=range(len(np.unique(y_encoded))))\n\ncbar.ax.set_yticklabels([emotion_mapping[i] for i in sorted(emotion_mapping)])\n\nplt.title('EMBEDDINGS REDUCED TO 2 DIMENSIONS')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\n# in 3d, had to look this one up unfortunately but it looks so cool\ntsne_3d = TSNE(n_components=3, random_state=42)\nX_reduced_3d = tsne_3d.fit_transform(X_embed)\n\nfig = px.scatter_3d(\n    x=X_reduced_3d[:, 0],\n    y=X_reduced_3d[:, 1],\n    z=X_reduced_3d[:, 2],\n    color=emotion_labels[y_encoded],\n    title='3D NOW???'\n)\n\nfig.update_traces(marker=dict(size=5))\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}