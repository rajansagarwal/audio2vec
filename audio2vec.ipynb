{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import os\n","import librosa\n","import numpy as np\n","\n","def load_and_process_audio(file_path, max_pad_len=None):\n","    # load a .wav file with sampling rate\n","    audio, sr = librosa.load(file_path, sr=22050)\n","    \n","    # spectrogram\n","    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n","    \n","    # convert to db\n","    S_dB = librosa.power_to_db(S, ref=np.max)\n","    \n","    # padding & tripping the calculated spectrogram!\n","    if max_pad_len:\n","        # this part confused me at first\n","        # we are just normalizing! \n","        # making it smaller or larger as needed\n","        pad_width = max_pad_len - S_dB.shape[1]\n","        # pad if smaller\n","        if pad_width > 0:\n","            S_dB = np.pad(S_dB, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","        # trim if larger\n","        else:\n","            S_dB = S_dB[:, :max_pad_len]\n","    \n","    # normalize dimensions\n","    S_dB = S_dB[..., np.newaxis]\n","    return S_dB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from dataset\n","max_pad_len = 228"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","def build_model(input_shape, dimensions):\n","    model = tf.keras.Sequential([\n","        # lots of stuff happening here!\n","        # we start with messy spectrogram data, but want to end up with a dimensional space\n","        \n","        # first round of feature extraction\n","        # capture low-level spatial features (ie textures)\n","        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n","\n","        # reduces spatial dimensions for next layer\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n","\n","        # second round of feature extraction, more complex\n","        # capture more complex spatial features\n","        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","\n","        # reduce spatial dimensions again\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n","        \n","        # now in the format we need\n","        # convert 2d feature maps into 1d\n","        tf.keras.layers.Flatten(),\n","        \n","        # 1D representation -> embedding\n","        # represent embedding as dimensional vector\n","        tf.keras.layers.Dense(dimensions, activation='relu'),\n","        \n","        # different classifications of emotions (we have 8 of them!)\n","        tf.keras.layers.Dense(num_classes, activation='softmax')\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","\n","directory = \"ravdess/\"\n","\n","data = []\n","labels = []\n","for subdir in tqdm(os.listdir(directory)):\n","    subdir_path = os.path.join(directory, subdir)\n","    if os.path.isdir(subdir_path):\n","        # walk through all (thanks cursor for the help here)\n","        for filename in os.listdir(subdir_path):\n","            \n","            file_path = os.path.join(subdir_path, filename)\n","            \n","            if os.path.isfile(file_path) and file_path.endswith('.wav'):\n","                spectrogram = load_and_process_audio(file_path, max_pad_len)\n","                # based on filename format from dataset\n","                emotion = filename.split('-')[2]\n","                \n","                data.append(spectrogram)\n","                labels.append(emotion)\n","                \n","X = np.array(data)\n","y = np.array(labels)\n","\n","X.shape, X[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# similar approach to contrastive learning!\n","# we want to encode data based on our labels\n","# that's kinda the point of this all\n","\n","encoder = LabelEncoder()\n","y_encoded = encoder.fit_transform(y)\n","y_categorical = to_categorical(y_encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input_shape = (128, max_pad_len, 1)\n","num_classes = len(np.unique(y_encoded))\n","dimensions = 256\n","\n","model = build_model(input_shape, dimensions)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(X, y_categorical, epochs=10, batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# given from dataset!\n","\n","emotion_mapping = {\n","    1: 'neutral',\n","    2: 'calm',\n","    3: 'happy',\n","    4: 'sad',\n","    5: 'angry',\n","    6: 'fearful',\n","    7: 'disgust',\n","    8: 'surprised'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","\n","# initialize model to extract features like input and layers\n","_ = model.predict(X)\n","\n","# dense layer\n","embeddings = model.layers[-2].output\n","\n","# run model on our initial data\n","model_embedding = tf.keras.Model(inputs=model.input, outputs=embeddings)\n","X_embed = model_embedding.predict(X)\n","\n","# reduce down to 2dim space\n","tsne = TSNE(n_components=2, random_state=42)\n","X_reduced = tsne.fit_transform(X_embed)\n","\n","plt.figure(figsize=(10, 8))\n","scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_encoded, cmap='viridis')\n","\n","cbar = plt.colorbar(scatter, ticks=range(len(np.unique(y_encoded))))\n","\n","cbar.ax.set_yticklabels([emotion_mapping[i] for i in sorted(emotion_mapping)])\n","\n","plt.title('EMBEDDINGS REDUCED TO 2 DIMENSIONS')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import plotly.express as px\n","\n","# in 3d, had to look this one up unfortunately but it looks so cool\n","tsne_3d = TSNE(n_components=3, random_state=42)\n","X_reduced_3d = tsne_3d.fit_transform(X_embed)\n","\n","fig = px.scatter_3d(\n","    x=X_reduced_3d[:, 0],\n","    y=X_reduced_3d[:, 1],\n","    z=X_reduced_3d[:, 2],\n","    color=emotion_labels[y_encoded],\n","    title='3D NOW???'\n",")\n","\n","fig.update_traces(marker=dict(size=5))\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":107620,"sourceId":256618,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
